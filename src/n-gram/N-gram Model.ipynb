{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokenized Words for Bad Movies: 806955\n",
      "Number of Tokenized Words for Good Movies: 1119560\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "import os\n",
    "\n",
    "# Get the number format such as: 001, 002, 010, 100\n",
    "def getNum(i):\n",
    "    num = \"\"\n",
    "    if len(str(i)) == 1:\n",
    "        num = \"00\" + str(i)\n",
    "    if len(str(i)) == 2:\n",
    "        num = \"0\" + str(i)\n",
    "    if len(str(i)) == 3:\n",
    "        num = str(i)\n",
    "    return num\n",
    "\n",
    "# Return a tokensized text of the Transcripts\n",
    "def getMovieTranscripts(low = True):\n",
    "    movieScripts = []\n",
    "    movieScores = \"\"\n",
    "    if low == True:\n",
    "        movieScores = \"Bad\"\n",
    "    else:\n",
    "        movieScores = \"Good\"\n",
    "    #Taking advantage of the naming convension for the files, iterate through the files.\n",
    "    for i in list(range(1,101)):\n",
    "        #Check if the file exists.\n",
    "        if os.path.isfile('data\\\\Processed'+ movieScores + 'Scripts\\\\' + getNum(i) + '-transcript.txt'): \n",
    "            f = open('data\\\\Processed'+ movieScores + 'Scripts\\\\' + getNum(i) + '-transcript.txt', 'r')\n",
    "            content = f.read()\n",
    "            # Normalize the whitespaces and lowercase everything\n",
    "            newContent = \" \".join(content.lower().split())\n",
    "            movieScripts += nltk.word_tokenize(newContent)\n",
    "    return addStartTokens(movieScripts)\n",
    "\n",
    "# Adds a Start Token before each sentence. \n",
    "def addStartTokens(script):\n",
    "    newList = ['[START]']\n",
    "    count = 1\n",
    "    for token in script:\n",
    "        newList.append(token)  \n",
    "        if token in ['.', '!', '?']:\n",
    "            newList.insert(count + 1, '[START]')\n",
    "            count += 1\n",
    "        count += 1\n",
    "    if newList[-1] == '[START]':\n",
    "        newList.pop()\n",
    "    return newList\n",
    "\n",
    "badMovieScripts = getMovieTranscripts(low = True)\n",
    "print(\"Number of Tokenized Words for Bad Movies:\", len(badMovieScripts))\n",
    "\n",
    "goodMovieScripts = getMovieTranscripts(low = False)\n",
    "print(\"Number of Tokenized Words for Good Movies:\", len(goodMovieScripts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Dictionary\n",
    "\n",
    "Create a dictionary such that calling dictionary['[START]'] returns a dictionary of all of the words that came after it along with its counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] oh 1893\n",
      "[START] you 4665\n",
      "[START] what 2739\n",
      "[START] that 1480\n",
      "[START] it 2441\n",
      "[START] got 97\n",
      "[START] i 9058\n",
      "[START] get 557\n",
      "[START] is 375\n",
      "[START] ready 31\n",
      "[START] three 35\n",
      "[START] things 18\n",
      "[START] whoa 134\n",
      "[START] u.s. 3\n",
      "[START] they 809\n",
      "[START] a 673\n",
      "[START] amy 13\n",
      "[START] piss 2\n",
      "[START] listen 163\n",
      "[START] when 187\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def createBigramCount(scripts):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    bigrams = ngrams(scripts, 2)\n",
    "    for i in scripts:\n",
    "        for word1, word2 in bigrams:\n",
    "            model[word1][word2] += 1\n",
    "    return model\n",
    "\n",
    "model = createBigramCount(badMovieScripts)\n",
    "#model['[START]']\n",
    "\n",
    "#The dictionary is structured like this: [word1 : [word2: Count(word2)]]\n",
    "count = 0\n",
    "for word2 in model['[START]']:\n",
    "    if count < 20:\n",
    "        print('[START]', word2, model['[START]'][word2])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model\n",
    "\n",
    "Given the movies script, the starting word, number of sentences, and alpha value, generate its own sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bigramPredict(scripts, word, length, alpha):\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    sentence = word.split()\n",
    "    #print(np.random.choice(list(model[sentence[-1]]), 1, p = [float(i)/sum(model[sentence[-1]].values()) for i in model[sentence[-1]].values()]))\n",
    "    count = 0\n",
    "    # Punctuations will be used to determine the ending of a sentence.\n",
    "    punctuations = ['.', '!', '?']\n",
    "    while count < length:\n",
    "        #sentence.append(max(bigramModel[sentence[-1]], key=bigramModel[sentence[-1]].get))  \n",
    "        randomToken = np.random.choice(list(bigramModel[sentence[-1]]), 1, p = [float(i + alpha)/(sum(bigramModel[sentence[-1]].values()) + (alpha * len(bigramModel[sentence[-1]].keys()))) for i in bigramModel[sentence[-1]].values()])[0]\n",
    "        if randomToken in punctuations:\n",
    "            count += 1\n",
    "        sentence.append(randomToken)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Dictionary\n",
    "\n",
    "Create a dictionary such that calling dictionary['[START]', 'oh'] returns a dictionary of all of the words that came after these two pairs along with its counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest count after 'you, just' is:  ,\n",
      "('[START]', 'oh') , 1415\n",
      "('[START]', 'oh') no 5\n",
      "('[START]', 'oh') my 46\n",
      "('[START]', 'oh') how 1\n",
      "('[START]', 'oh') god 11\n",
      "('[START]', 'oh') . 140\n",
      "('[START]', 'oh') yeah 19\n",
      "('[START]', 'oh') right 2\n",
      "('[START]', 'oh') dropped 1\n",
      "('[START]', 'oh') on 1\n",
      "('[START]', 'oh') well 3\n",
      "('[START]', 'oh') coopie 1\n",
      "('[START]', 'oh') hey 1\n",
      "('[START]', 'oh') and 2\n",
      "('[START]', 'oh') hi 3\n",
      "('[START]', 'oh') listen 1\n",
      "('[START]', 'oh') really 1\n",
      "('[START]', 'oh') look 2\n",
      "('[START]', 'oh') shit 6\n",
      "('[START]', 'oh') fuck 1\n",
      "('[START]', 'oh') ! 172\n",
      "('[START]', 'oh') harsh 1\n",
      "('[START]', 'oh') ohh 1\n",
      "('[START]', 'oh') all 1\n",
      "('[START]', 'oh') that 1\n",
      "('[START]', 'oh') simon 1\n",
      "('[START]', 'oh') ugly 1\n",
      "('[START]', 'oh') : 1\n",
      "('[START]', 'oh') hello 1\n",
      "('[START]', 'oh') thanks 1\n",
      "('[START]', 'oh') cute 1\n",
      "('[START]', 'oh') oh 3\n",
      "('[START]', 'oh') wait 1\n",
      "('[START]', 'oh') love 1\n",
      "('[START]', 'oh') we 2\n",
      "('[START]', 'oh') were 1\n",
      "('[START]', 'oh') yes 3\n",
      "('[START]', 'oh') now 2\n",
      "('[START]', 'oh') barb 1\n",
      "('[START]', 'oh') ? 5\n",
      "('[START]', 'oh') boy 2\n",
      "('[START]', 'oh') i 4\n",
      "('[START]', 'oh') right.. 1\n",
      "('[START]', 'oh') snap 1\n",
      "('[START]', 'oh') yea 1\n",
      "('[START]', 'oh') great 2\n",
      "('[START]', 'oh') peter 1\n",
      "('[START]', 'oh') ah 1\n",
      "('[START]', 'oh') what 1\n",
      "('[START]', 'oh') get 1\n",
      "('[START]', 'oh') it 2\n",
      "('[START]', 'oh') very 1\n",
      "('[START]', 'oh') okay 1\n",
      "('[START]', 'oh') ready 1\n",
      "('[START]', 'oh') if 1\n",
      "('[START]', 'oh') dear 1\n",
      "('[START]', 'oh') fred 2\n",
      "('[START]', 'oh') dumdum 1\n",
      "('[START]', 'oh') you 1\n",
      "('[START]', 'oh') ' 1\n",
      "('[START]', 'oh') forget 1\n",
      "('[START]', 'oh') weird 1\n",
      "('[START]', 'oh') thank 1\n"
     ]
    }
   ],
   "source": [
    "def createTrigramCount(scripts):\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    trigrams = ngrams(scripts, 3)\n",
    "    for i in scripts:\n",
    "        for word1, word2, word3 in trigrams:\n",
    "            model[(word1, word2)][word3] += 1\n",
    "    return model\n",
    "\n",
    "model = createTrigramCount(badMovieScripts)\n",
    "#The dictionary is structured like this: [(word1, word2) : [word 3: Count(word3)]]\n",
    "word1_word2 = ('[START]', 'oh')\n",
    "print(\"The highest count after 'you, just' is: \", max(model[word1_word2], key=model[word1_word2].get))\n",
    "for word3 in model[word1_word2]:\n",
    "    print(word1_word2, word3, model[word1_word2][word3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram Model\n",
    "\n",
    "Given the movies script, the starting word, number of sentences, and alpha value, generate its own sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def trigramPredict(scripts, word, length, alpha):\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    trigramModel = createTrigramCount(scripts)\n",
    "    sentence = []\n",
    "    sentence.append(word)\n",
    "    sentence.append(np.random.choice(list(bigramModel[word]), 1, p = [float(i + alpha)/(sum(bigramModel[word].values()) + (alpha * len(bigramModel[word].keys()))) for i in bigramModel[word].values()])[0])\n",
    "    count = 0\n",
    "    punctuations = ['.', '!', '?']\n",
    "    while count < length:\n",
    "        if len(trigramModel[(sentence[-2], sentence[-1])]) == 0:\n",
    "            pick = np.random.choice(list(bigramModel[sentence[-1]]), 1, p = [float(i + alpha)/(sum(bigramModel[sentence[-1]].values()) + (alpha * len(bigramModel[sentence[-1]].keys()))) for i in bigramModel[sentence[-1]].values()])[0]\n",
    "            if pick in punctuations:\n",
    "                count += 1\n",
    "            sentence.append(pick)\n",
    "        else:\n",
    "            #sentence.append(max(trigramModel[(sentence[-2], sentence[-1])], key=trigramModel[(sentence[-2], sentence[-1])].get))\n",
    "            randomToken = np.random.choice(list(trigramModel[(sentence[-2], sentence[-1])]), 1, p = [float(i + alpha)/(sum(trigramModel[(sentence[-2], sentence[-1])].values()) + (alpha * len(trigramModel[(sentence[-2], sentence[-1])].keys()))) for i in trigramModel[(sentence[-2], sentence[-1])].values()])[0]\n",
    "            if randomToken in punctuations:\n",
    "                count += 1\n",
    "            sentence.append(randomToken)\n",
    "            \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Sentences\n",
    "\n",
    "To make the sentences presentable, remove the start token, and correct the spacings in between words and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Bigram - Bad Movies ###\n",
      "''lovecake'' she's being so inconspicuous and skilled enough for this to. Come on, I missed. I'm not remember. You. They are showing off.\n",
      "\n",
      "### Bigram - Good Movies ###\n",
      "Fuck what was determined to. Go. Let's right here. That glowers in nepal? Alex.\n",
      "\n",
      "### Trigram - Bad Movies ###\n",
      "Are you kidding me. Then the water. If your interests are noble, but I want save passage for me? Come on! He once killed his own daughter betraying me to say everything twice.\n",
      "\n",
      "### Trigram - Good Movies ###\n",
      "What could they be a woman soon soon you gon na make anyone'pay'. Hotel freezer day m.s. I wanted to get into character. Men have emptied entire clips at them the devil do you say yes! You mind if I ever thank you, sir.\n"
     ]
    }
   ],
   "source": [
    "def generate(ls):\n",
    "    output = ls[1].capitalize()\n",
    "    for token in ls[2:]:\n",
    "        if token == '[START]':\n",
    "            pass\n",
    "        elif output[-1] in ['.', '!', '?'] or token == 'i':\n",
    "            output += \" \" + token.capitalize()\n",
    "        elif token in [',', ':', '.', '!', '?'] or token[0] == \"'\" or token == \"n't\":\n",
    "            output += token\n",
    "        else:\n",
    "            output += \" \" + token\n",
    "    return output\n",
    "\n",
    "print(\"### Bigram - Bad Movies ###\")\n",
    "print(generate(bigramPredict(badMovieScripts, '[START]', 5, 0.1)))\n",
    "print()\n",
    "print(\"### Bigram - Good Movies ###\")\n",
    "print(generate(bigramPredict(goodMovieScripts, '[START]', 5, 0.1)))\n",
    "print()\n",
    "print(\"### Trigram - Bad Movies ###\")\n",
    "print(generate(trigramPredict(badMovieScripts, '[START]', 5, 0.1)))\n",
    "print()\n",
    "print(\"### Trigram - Good Movies ###\")\n",
    "print(generate(trigramPredict(goodMovieScripts, '[START]', 5, 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of Sentences\n",
    "\n",
    "Calculate the probability of a sentence being generated by the n-gram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: Prob( [START] you don't worry. ) =  2.341232113375408e-07\n",
      "Trigram: Prob( [START] you don't worry. ) =  0.007246376811594203\n"
     ]
    }
   ],
   "source": [
    "def bigramProb(sentence, scripts, alpha):\n",
    "    s = sentence.split()\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    prob = float(bigramModel[s[0]][s[1]] + alpha)/(sum(bigramModel[s[0]].values()) + (alpha * len(bigramModel[s[0]].keys())))\n",
    "    index = 1\n",
    "#     print(prob)\n",
    "    for word in s[2:]:\n",
    "        product = float(bigramModel[s[index]][word] + alpha)/(sum(bigramModel[s[index]].values()) + (alpha * len(bigramModel[s[index]].keys())))\n",
    "#         print(product)\n",
    "        prob *= product\n",
    "        index += 1\n",
    "    return prob\n",
    "\n",
    "\n",
    "def trigramProb(sentence, scripts, alpha):\n",
    "    s = sentence.split()\n",
    "    bigramModel = createBigramCount(scripts)\n",
    "    prob = float(bigramModel[s[0][1]][s[2]] + alpha)/(sum(bigramModel[s[0][1]].values()) + (alpha * len(bigramModel[s[0][1]].keys())))\n",
    "    index = 1\n",
    "#     print(prob)\n",
    "    for word in s[3:]:\n",
    "        product = float(bigramModel[s[index][index + 1]][word] + alpha)/(sum(bigramModel[s[index][index + 1]].values()) + (alpha * len(bigramModel[s[index][index + 1]].keys())))\n",
    "#         print(product)\n",
    "        prob *= product\n",
    "        index += 1\n",
    "    return prob\n",
    "\n",
    "# EXAMPLE\n",
    "s = \"[START] you don't worry.\"\n",
    "print(\"Bigram: Prob(\", s, \") = \", bigramProb(s, badMovieScripts, 0.1))\n",
    "print(\"Trigram: Prob(\", s, \") = \", trigramProb(s, badMovieScripts, 0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
